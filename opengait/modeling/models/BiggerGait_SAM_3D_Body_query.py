# Copyright (c) Meta Platforms, Inc. and affiliates.
# All rights reserved.

import sys
import os
import torch
import torch.nn as nn
import torch.utils.checkpoint
from einops import repeat,rearrange
from ..base_model import BaseModel
from torch.nn import functional as F
from kornia import morphology as morph
import random
from functools import partial

# import GaitBase
from .BigGait_utils.BigGait_GaitBase import *
from .BigGait_utils.save_img import save_image, pca_image
from ..modules import GaitAlign

# =========================================================================
# Helper Functions (Keep same as CLIP version)
# =========================================================================

def gradient_hook(grad, name, step, log):
    if torch.distributed.get_rank() == 0 and step % 100 == 0:
        log.log_info('[{}] Gradient={:.6f}'.format(step, grad.abs().mean().item()))
    return grad

def center_masked_kernel(K, mask_flat):
    N, HW, _ = K.shape
    M = mask_flat.sum(dim=1, keepdim=True).unsqueeze(2).float()
    M = torch.where(M == 0, torch.ones_like(M), M)
    sum_rows = torch.sum(K, dim=2, keepdim=True)
    sum_cols = torch.sum(K, dim=1, keepdim=True)
    row_means = (sum_rows / M) * mask_flat.unsqueeze(2)
    col_means = (sum_cols / M) * mask_flat.unsqueeze(1)
    total_mean = torch.sum(K, dim=(1, 2), keepdim=True) / (M ** 2)
    K_centered = K - row_means - col_means + total_mean
    mask_matrix = mask_flat.unsqueeze(2) * mask_flat.unsqueeze(1)
    return K_centered * mask_matrix.float()

class infoDistillation(nn.Module):
    def __init__(self, source_dim, target_dim, p, softmax, Relu, Up=True):
        super(infoDistillation, self).__init__()
        self.dropout = nn.Dropout(p=p)
        self.bn_s = nn.BatchNorm1d(source_dim, affine=False)
        self.bn_t = nn.BatchNorm1d(target_dim, affine=False)
        if Relu:
            self.down_sampling = nn.Sequential(
                nn.Linear(source_dim, source_dim//2),
                nn.BatchNorm1d(source_dim//2, affine=False),
                nn.GELU(),
                nn.Linear(source_dim//2, target_dim),
                )
            if Up:
                self.up_sampling = nn.Sequential(
                    nn.Linear(target_dim, source_dim//2),
                    nn.BatchNorm1d(source_dim//2, affine=False),
                    nn.GELU(),
                    nn.Linear(source_dim//2, source_dim),
                    )
        else:
            self.down_sampling = nn.Linear(source_dim, target_dim)
            if Up:
                self.up_sampling = nn.Linear(target_dim, source_dim)
        self.softmax = softmax
        self.mse = nn.MSELoss()
        self.Up = Up

    def forward(self, x):
        d_x = self.down_sampling(self.bn_s(self.dropout(x)))
        if self.softmax:
            d_x = F.softmax(d_x, dim=1)
            if self.Up:
                u_x = self.up_sampling(d_x)
                return d_x, torch.mean(self.mse(u_x, x))
            else:
                return d_x, None
        else:
            if self.Up:
                u_x = self.up_sampling(d_x)
                return torch.sigmoid(self.bn_t(d_x)), torch.mean(self.mse(u_x, x))
            else:
                return torch.sigmoid(self.bn_t(d_x)), None

class ResizeToHW(torch.nn.Module):
    def __init__(self, target_size):
        super().__init__()
        self.target_size = target_size

    def forward(self, x):
        return F.interpolate(x, size=self.target_size, mode='bilinear', align_corners=False)

# =========================================================================
# Main Model: BiggerGait with SAM 3D Body (DINOv3)
# =========================================================================

class BiggerGait__SAM3DBody__Query_Gaitbase_Share(BaseModel):
    def build_network(self, model_cfg):
        # 1. åŸºç¡€å‚æ•°
        self.pretrained_lvm = model_cfg["pretrained_lvm"]
        self.pretrained_mask_branch = model_cfg["pretrained_mask_branch"]
        self.image_size = model_cfg["image_size"]
        self.sils_size = model_cfg["sils_size"]
        self.f4_dim = model_cfg["source_dim"]
        self.num_unknown = model_cfg["num_unknown"]
        self.num_FPN = model_cfg["num_FPN"]

        # ====================================================
        # ğŸŒŸ [é€»è¾‘ä¿®æ”¹] è§£æå±‚é…ç½®ä¸æ•°å­¦éªŒè¯
        # ====================================================
        layer_cfg = model_cfg.get("layer_config", {})
        self.layers_per_group = layer_cfg.get("layers_per_group", 2)
        
        # 1. è·å–/ç”Ÿæˆ Hook Mask (32å±‚)
        if "hook_mask" in layer_cfg:
            self.hook_mask = layer_cfg["hook_mask"]
            if len(self.hook_mask) != 32:
                raise ValueError(f"hook_mask é•¿åº¦å¿…é¡»ä¸º 32ï¼Œå½“å‰ä¸º {len(self.hook_mask)}")
        else:
            # é»˜è®¤ï¼šå‰16å±‚ Falseï¼Œå16å±‚ True
            self.hook_mask = [False]*16 + [True]*16
            self.msg_mgr.log_info("[Network] No hook_mask found, using default (Top-16).")

        # 2. è®¡ç®—å®é™… Hook çš„å±‚æ•°
        self.total_hooked_layers = sum(self.hook_mask)
        self.msg_mgr.log_info(f"[Network] Total Layers to Hook: {self.total_hooked_layers}")

        if self.total_hooked_layers == 0:
            raise ValueError("hook_mask å…¨ä¸º Falseï¼Œæ²¡æœ‰å±‚è¢«é€‰ä¸­ï¼")

        # 3. [éªŒè¯] èƒ½å¦è¢« group æ•´é™¤
        if self.total_hooked_layers % self.layers_per_group != 0:
            raise ValueError(f"Hookæ€»å±‚æ•° ({self.total_hooked_layers}) æ— æ³•è¢« layers_per_group ({self.layers_per_group}) æ•´é™¤ï¼")
        
        self.total_groups = self.total_hooked_layers // self.layers_per_group
        self.msg_mgr.log_info(f"[Network] Total Groups: {self.total_groups} (Size: {self.layers_per_group})")

        # 4. [éªŒè¯] æ€» Group èƒ½å¦è¢« num_FPN æ•´é™¤
        if self.total_groups % self.num_FPN != 0:
            raise ValueError(f"æ€»Groupæ•° ({self.total_groups}) æ— æ³•è¢« num_FPN ({self.num_FPN}) æ•´é™¤ï¼")

        # 5. è®¡ç®—æ¯ä¸ª Head è´Ÿè´£å¤„ç†å‡ å±‚ (Layers Per Head)
        # è¿™å†³å®šäº† HumanSpace_Conv çš„è¾“å…¥é€šé“æ•°
        # é€»è¾‘ï¼šæ€»å±‚æ•° / FPNæ•°
        self.layers_per_head = self.total_hooked_layers // self.num_FPN
        input_dim = self.f4_dim * self.layers_per_head
        
        self.msg_mgr.log_info(f"[Network] === Configuration Validated ===")
        self.msg_mgr.log_info(f"          |-> FPN Heads: {self.num_FPN}")
        self.msg_mgr.log_info(f"          |-> Layers per Head: {self.layers_per_head}")
        self.msg_mgr.log_info(f"          |-> Conv Input Dim: {input_dim}")
        # ====================================================

        self.chunk_size = model_cfg.get("chunk_size", 96)

        # åˆå§‹åŒ–ä¸‹æ¸¸ç½‘ç»œ
        # self.Gait_Net = Baseline_ShareTime_2B(model_cfg)
        self.Gait_Net = Baseline_Semantic_2B(model_cfg)
        self.Pre_Conv = nn.Sequential(nn.Identity())

        # FPN é€‚é…å±‚ (æ ¹æ®è®¡ç®—å‡ºçš„ input_dim åˆå§‹åŒ–)
        self.HumanSpace_Conv = nn.ModuleList([
            nn.Sequential(
                nn.BatchNorm2d(input_dim, affine=False),
                nn.Conv2d(input_dim, self.f4_dim//2, kernel_size=1),
                nn.BatchNorm2d(self.f4_dim//2, affine=False),
                nn.GELU(),
                nn.Conv2d(self.f4_dim//2, self.num_unknown, kernel_size=1),
                ResizeToHW((self.sils_size*2, self.sils_size)),
                nn.BatchNorm2d(self.num_unknown, affine=False),
                nn.Sigmoid()
            ) for _ in range(self.num_FPN)
        ])
        
        self.Mask_Branch = infoDistillation(**model_cfg["Mask_Branch"])
        
        self.t_channel = self.f4_dim
        self.temb_proj = nn.Sequential(
            nn.Linear(self.t_channel, self.t_channel),
            nn.ReLU(),
            nn.Linear(self.t_channel, self.t_channel),
        )

    def init_SAM_Backbone(self):
        if self.pretrained_lvm not in sys.path:
            sys.path.insert(0, self.pretrained_lvm)
        
        try:
            from notebook.utils import setup_sam_3d_body
        except ImportError as e:
            raise ImportError(f"Cannot import setup_sam_3d_body. Error: {e}")

        self.msg_mgr.log_info(f"[SAM3D] Loading SAM 3D Body...")
        estimator = setup_sam_3d_body(hf_repo_id="facebook/sam-3d-body-dinov3", device='cpu')
        
        self.SAM_Engine = estimator.model
        
        try:
            self.Backbone = self.SAM_Engine.backbone.encoder
            self.msg_mgr.log_info(f"[SAM3D] Backbone set to SAM_Engine.backbone.encoder")
        except AttributeError:
            raise RuntimeError("Could not find Backbone encoder in SAM Engine")

        try:
            self.Decoder = self.SAM_Engine.decoder
            self.msg_mgr.log_info(f"[SAM3D] Decoder set to SAM_Engine.decoder")
            # å¼ºåˆ¶å…³é—­ä¸­é—´ç›‘ç£ï¼Œé˜²æ­¢å› ç¼ºå°‘å›è°ƒå‡½æ•°è€Œ Crash
            self.Decoder.do_interm_preds = False
            # å¼ºåˆ¶å…³é—­ Keypoint Token Update (å¦‚æœæœ‰çš„è¯)ï¼ŒåŒæ ·é˜²æ­¢ Crash
            self.Decoder.keypoint_token_update = False
        except AttributeError:
            raise RuntimeError("Could not find Decoder in SAM Engine")
        
        # 4. æå– Keypoint Queries (Parts)
        # ç›´æ¥æå–é¢„è®­ç»ƒå¥½çš„ Embeddings
        # shape: [70, Dim]
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ .clone() å¹¶ä¸”æ³¨å†Œä¸º Bufferï¼Œæ„å‘³ç€æˆ‘ä»¬å°†å…¶è§†ä¸º"å¸¸é‡å…ˆéªŒ"ï¼Œä¸è¿›è¡Œæ›´æ–°
        # å¦‚æœä½ æƒ³å¾®è°ƒè¿™äº› Queryï¼Œå¯ä»¥æ”¹ä¸º nn.Parameter
        part_query_data = self.SAM_Engine.keypoint_embedding.weight.data.clone()
        self.register_buffer('fixed_part_queries', part_query_data)

        # =======================================================
        # ğŸŒŸ [å…³é”®ä¿®æ­£] æå– K_proj (å›¾åƒæŠ•å½±) å’Œ Q_proj (è¯­ä¹‰æŠ•å½±)
        # =======================================================
        self.feat_k_proj = None
        self.feat_q_proj = None
        
        try:
            # è·¯å¾„: Decoder -> æœ€åä¸€å±‚ -> Cross Attention
            last_layer = self.Decoder.layers[-1]
            
            if hasattr(last_layer, 'cross_attn'):
                cross_attn = last_layer.cross_attn
                
                # 1. æå– K_proj (1280 -> 512)
                if hasattr(cross_attn, 'k_proj'):
                    self.feat_k_proj = cross_attn.k_proj
                
                # 2. æå– Q_proj (1024 -> 512)
                if hasattr(cross_attn, 'q_proj'):
                    self.feat_q_proj = cross_attn.q_proj
                    
                if self.feat_k_proj and self.feat_q_proj:
                    self.msg_mgr.log_info(f"[SAM3D] Reusing Projections: K={self.feat_k_proj}, Q={self.feat_q_proj}")
                else:
                    raise AttributeError("Missing k_proj or q_proj in CrossAttn.")
            else:
                raise AttributeError("No cross_attn found.")
                
        except Exception as e:
            self.msg_mgr.log_warning(f"[SAM3D] Projection extraction failed: {e}")
            raise RuntimeError("Failed to extract k_proj and q_proj from Decoder.")
        
        self.msg_mgr.log_info(f"[SAM3D] Extracted {self.fixed_part_queries.shape[0]} Part Queries. (No MHR)")

        # 5. æ³¨å†Œ FPN Hook (ä¿æŒä¸å˜)
        self.intermediate_features = {}
        self.hook_handles = []
        
        def get_activation(idx_in_list):
            def hook(model, input, output):
                if isinstance(output, (list, tuple)): output = output[0]
                self.intermediate_features[idx_in_list] = output
            return hook

        target_blocks = None
        if hasattr(self.Backbone, 'blocks'):
            target_blocks = self.Backbone.blocks
        elif hasattr(self.Backbone, 'layers'):
            target_blocks = self.Backbone.layers
        
        if target_blocks:
            hook_count = 0
            for layer_idx, should_hook in enumerate(self.hook_mask):
                if should_hook and layer_idx < len(target_blocks):
                    target_blocks[layer_idx].register_forward_hook(get_activation(hook_count))
                    self.hook_handles.append(None)
                    hook_count += 1
            self.msg_mgr.log_info(f"[SAM3D] Hooked {hook_count} layers inside Backbone.")

        # 6. å†»ç»“æ‰€æœ‰ç»„ä»¶
        self.SAM_Engine.eval()
        for param in self.SAM_Engine.parameters():
            param.requires_grad = False
            
        # æ¸…ç†
        del estimator

    def init_Mask_Branch(self): # TODO
        # self.msg_mgr.log_info(f'load model from: {self.pretrained_mask_branch}')
        # load_dict = torch.load(self.pretrained_mask_branch, map_location=torch.device("cpu"))['model']
        # msg = self.Mask_Branch.load_state_dict(load_dict, strict=True)
        # n_parameters = sum(p.numel() for p in self.Mask_Branch.parameters())
        # self.msg_mgr.log_info('Missing keys: {}'.format(msg.missing_keys))
        # self.msg_mgr.log_info('Unexpected keys: {}'.format(msg.unexpected_keys))
        # self.msg_mgr.log_info(f"=> loaded successfully '{self.pretrained_mask_branch}'")
        # self.msg_mgr.log_info('SegmentationBranch Count: {:.5f}M'.format(n_parameters / 1e6))

        # åŸæ¥çš„ä»£ç æ˜¯åŠ è½½ .pt æ–‡ä»¶ï¼Œç°åœ¨ç›´æ¥è·³è¿‡
        self.msg_mgr.log_info("=> Skip loading Mask Branch (Using Full Image Features)")
        # ä¿æŒ Mask_Branch ä¸ºéšæœºåˆå§‹åŒ–å³å¯ï¼Œåæ­£ forward é‡Œæˆ‘ä»¬ä¸ç”¨å®ƒ
        pass

    def init_parameters(self):
        for m in self.modules():
            if isinstance(m, (nn.Conv3d, nn.Conv2d, nn.Conv1d)):
                nn.init.xavier_uniform_(m.weight.data)
                if m.bias is not None:
                    nn.init.constant_(m.bias.data, 0.0)
            elif isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight.data)
                if m.bias is not None:
                    nn.init.constant_(m.bias.data, 0.0)
            elif isinstance(m, (nn.BatchNorm3d, nn.BatchNorm2d, nn.BatchNorm1d)):
                if m.affine:
                    nn.init.normal_(m.weight.data, 1.0, 0.02)
                    nn.init.constant_(m.bias.data, 0.0)

        self.init_SAM_Backbone()
        self.init_Mask_Branch()
        
        # ç¡®ä¿ Mask Branch å’Œ Backbone æ˜¯å†»ç»“çš„
        self.Backbone.eval()
        self.Backbone.requires_grad_(False)
        self.Mask_Branch.eval()
        self.Mask_Branch.requires_grad_(False)

        n_parameters = sum(p.numel() for p in self.parameters())
        self.msg_mgr.log_info('All Model Count: {:.5f}M'.format(n_parameters / 1e6))
        self.msg_mgr.log_info("=> init successfully")

    def preprocess(self, sils, h, w, mode='bilinear'):
        # å¼ºåˆ¶ Resize
        return F.interpolate(sils, (h, w), mode=mode, align_corners=False)

    def min_max_norm(self, x):
        return (x - x.min())/(x.max() - x.min())

    def forward(self, inputs):
        # ç¡®ä¿ Mask Branch ä¸æ›´æ–°
        self.Mask_Branch.eval()
        
        ipts, labs, ty, vi, seqL = inputs
        rgb = ipts[0]
        del ipts

        # æ˜¾å­˜ä¼˜åŒ–ï¼šå¦‚æœä¾ç„¶ OOMï¼Œå°†æ­¤å€¼æ”¹ä¸º 2
        CHUNK_SIZE = self.chunk_size
        rgb_chunks = torch.chunk(rgb, (rgb.size(1)//CHUNK_SIZE)+1, dim=1)
        
        all_outs = []
        # [æ–°å¢] ç”¨äºæ”¶é›†æ¯ä¸ª Chunk çš„ Attention Map
        all_attn_maps = []
        
        target_h, target_w = self.image_size * 2, self.image_size
        h_feat, w_feat = target_h // 16, target_w // 16

        for _, rgb_img in enumerate(rgb_chunks):
            n, s, c, h, w = rgb_img.size()
            rgb_img = rearrange(rgb_img, 'n s c h w -> (n s) c h w').contiguous()
            
            # =======================================================
            # 1. Backbone å‰å‘ (å¿…é¡»åŠ  no_grad ä»¥èŠ‚çœæ˜¾å­˜)
            # =======================================================
            with torch.no_grad():
                outs = self.preprocess(rgb_img, target_h, target_w) # ns c H W -> B 3 512 256
                
                # æ¸…ç©º Hook ç¼“å­˜
                self.intermediate_features = {}
                
                # DINOv3 æ¨ç†
                _ = self.Backbone(outs) # lay B L C -> 16 B 32*16+1+4=517 1280

                # [å…³é”®ä¿®æ”¹] ä» Hook ä¸­æå–æœ€åä¸€å±‚ç‰¹å¾ (3D Tensor)
                # self.hook_handles è®°å½•äº†æ³¨å†Œäº†å¤šå°‘ä¸ª hook
                # å¯¹åº”çš„ key æ˜¯ 0 åˆ° len-1
                # backbone_feat_3d: [B, L, Dim] (ä¾‹å¦‚ [4, 650, 1280])
                backbone_feat = self.intermediate_features[len(self.hook_handles) - 1]
                
                # =======================================================
                #  æ‰‹åŠ¨ç»„è£… Decoder è¾“å…¥ (Parts Only)
                # =======================================================
                
                # A. å‡†å¤‡ Image Embedding: [B, Dim, H_feat, W_feat]
                # è®¡ç®— Patch æ•°é‡
                # DINOv3 ViT-Huge é€šå¸¸ patch_size = 16
                patch_size = 16 
                feat_h = target_h // patch_size
                feat_w = target_w // patch_size
                num_spatial = feat_h * feat_w
                
                # æˆªå– Spatial Tokens (å»æ‰å‰é¢çš„ Register Tokens)
                # å¦‚æœ backbone_feat é•¿åº¦æ­£å¥½ç­‰äº num_spatialï¼Œè¯´æ˜æ²¡æœ‰ register token
                # å¦‚æœå¤§äºï¼Œè¯´æ˜å‰é¢æœ‰ register token
                if backbone_feat.shape[1] >= num_spatial:
                    img_tokens = backbone_feat[:, -num_spatial:, :] # å–æœ€å N ä¸ª

                # Reshape ä¸º Decoder è¦æ±‚çš„ [B, C, H, W]
                img_emb = rearrange(img_tokens, 'b (h w) c -> b c h w', h=feat_h, w=feat_w)
                
                # B. å‡†å¤‡ Token Embedding (Queries): [B, N_queries, Dim]
                bs = img_emb.shape[0]
                
                # åªä½¿ç”¨ Part Queriesï¼Œæ‰©å±•åˆ° Batch ç»´åº¦
                # [70, Dim] -> [B, 70, Dim]
                all_queries = repeat(self.fixed_part_queries, 'n d -> b n d', b=bs)
                
                # C. è°ƒç”¨ Decoder
                try:
                    # PromptableDecoder åªéœ€è¦è¿™ä¸¤ä¸ªä¸»è¦å‚æ•°å°±èƒ½è·‘
                    # å®ƒå†…éƒ¨ä¼šåˆ©ç”¨ CrossAttention è®© queries å»æŸ¥è¯¢ image_embedding
                    semantic_out = self.Decoder(
                        token_embedding=all_queries,
                        image_embedding=img_emb
                    )
                    
                    # semantic_out: [B, 70, Dim]
                    # è¿™é‡Œçš„ 70 å°±æ˜¯ 70 ä¸ªå…³é”®ç‚¹å¯¹åº”çš„ç‰¹å¾
                    
                    if isinstance(semantic_out, tuple):
                        semantic_out = semantic_out[0]
                        
                    # 2. ğŸŒŸ è®¡ç®— Attention Map (Soft Semantic Mask)
                    # æˆ‘ä»¬è®¡ç®— æ›´æ–°åçš„Query ä¸ å›¾åƒç‰¹å¾ çš„ç›¸ä¼¼åº¦
                    # Map = Softmax(Q @ K.T / sqrt(dim))

                    # 2. ğŸŒŸ è®¡ç®— Attention Map (åŒæŠ•å½±ä¿®æ­£ç‰ˆ)
                    
                    # [Step 1] æŠ•å½± Image Key (K)
                    # [B, HW, 1280] -> [B, HW, 512]
                    k_feats = self.feat_k_proj(img_tokens)
                    
                    # [Step 2] æŠ•å½± Semantic Query (Q)
                    # [B, 70, 1024] -> [B, 70, 512]
                    q_feats = self.feat_q_proj(semantic_out)
                    
                    # [Step 3] éªŒè¯ç»´åº¦å¹¶è®¡ç®—
                    dim_k = k_feats.shape[-1]
                    dim_q = q_feats.shape[-1]
                    
                    if dim_k != dim_q:
                        raise ValueError(f"Proj dim mismatch: K={dim_k}, Q={dim_q}")

                    # Dot Product: [B, 70, 512] @ [B, 512, HW] -> [B, 70, HW]
                    raw_attn = torch.matmul(q_feats, k_feats.transpose(1, 2))
                    raw_attn = raw_attn / (dim_q ** 0.5)
                    
                    # Softmax & Reshape
                    attn_map = F.softmax(raw_attn, dim=-1) # [B, 70, HW]
                    attn_map_spatial = rearrange(attn_map, 'b p (h w) -> b p h w', h=feat_h, w=feat_w)
                    
                    # self.msg_mgr.log_info(f"Generated Attention Map: {attn_map_spatial.shape}")
                    # [æ–°å¢] æ”¶é›† Attention Map
                    # æˆ‘ä»¬éœ€è¦å°†å…¶ Reshape å› [n, p, s, h, w] ä»¥ä¾¿åç»­åœ¨ s ç»´åº¦æ‹¼æ¥
                    # æ³¨æ„ï¼šè¿™é‡Œå¿…é¡»ä½¿ç”¨å½“å‰ chunk çš„ s
                    current_map = rearrange(attn_map_spatial, '(n s) p h w -> n p s h w', n=n, s=s)
                    all_attn_maps.append(current_map)
                    
                except Exception as e:
                    self.msg_mgr.log_warning(f"Manual Decoder/Attention Failed: {type(e).__name__}: {e}")
                    print(f"Q: {semantic_out.shape}, K_raw: {img_tokens.shape}")
                
                # æ”¶é›†è¢« Hook çš„å±‚
                num_layers = len(self.hook_handles) # 16
                features_to_use = []
                target_tokens = h_feat * w_feat # 512
                
                for i in range(num_layers):
                    feat = self.intermediate_features[i] # B L C -> B 517 1280
                    # å»é™¤ CLS Token ç­‰å¤šä½™éƒ¨åˆ†ï¼Œåªä¿ç•™ Spatial Tokens
                    if feat.shape[1] > target_tokens:
                        feat = feat[:, -target_tokens:, :]
                    features_to_use.append(feat) # lay B L C -> 16 B 512 1280

            # =======================================================
            # 2. FPN ç»„å¤„ç† (Group Processing)
            # =======================================================
            processed_feat_list = []
            
            # è‡ªåŠ¨è®¡ç®—æ­¥é•¿ï¼šä¾‹å¦‚ Hookäº†16å±‚ï¼ŒFPNæœ‰4ä¸ªï¼Œåˆ™ step=4 (å³æ¯ç»„4å±‚)
            # è¿™ä¸ build_network ä¸­çš„ input_dim è®¡ç®—é€»è¾‘æ˜¯å®Œå…¨å¯¹åº”çš„
            step = len(features_to_use) // self.num_FPN # 16 / 4 = 4
            
            for i in range(self.num_FPN):
                # A. åˆ‡ç‰‡ï¼šå–å‡ºå½“å‰ Head è´Ÿè´£çš„é‚£å‡ å±‚
                start_idx = i * step
                end_idx = (i + 1) * step
                sub_feats = features_to_use[start_idx : end_idx]
                
                # B. æ‹¼æ¥ï¼šå°†è¿™å‡ å±‚æ‹¼åœ¨ä¸€èµ·
                # ç»´åº¦å˜åŒ–: [B, N, 1280] x step -> [B, N, 1280*step]
                sub_app = torch.concat(sub_feats, dim=-1) # B 512 1280*4=5120
                
                # C. è°ƒæ•´å½¢çŠ¶ä»¥è¿›è¡Œå·ç§¯ [B, C, H, W]
                sub_app = rearrange(sub_app, 'b (h w) c -> b c h w', h=h_feat).contiguous() # B 5120 32 16
                
                # D. Pre_Conv (Identity)
                sub_app = self.Pre_Conv(sub_app)
                
                # E. å±€éƒ¨ LayerNorm (é’ˆå¯¹å½“å‰ç»„çš„ç»´åº¦è¿›è¡Œå½’ä¸€åŒ–)
                sub_app = rearrange(sub_app, 'b c h w -> b (h w) c') # B 512 5120
                # è®¡ç®—å½“å‰ç»„çš„é€šé“æ•°ï¼Œä¾‹å¦‚ 1280 * 2 = 2560
                curr_dim = self.f4_dim * len(sub_feats) # 1280 * 4 = 5120
                sub_app = partial(nn.LayerNorm, eps=1e-6)(curr_dim, elementwise_affine=False)(sub_app)
                sub_app = rearrange(sub_app, 'b (h w) c -> b c h w', h=h_feat).contiguous() # B 5120 32 16
                
                # F. å–‚ç»™ç¬¬ i ä¸ªç‹¬ç«‹çš„ FPN Head
                # self.HumanSpace_Conv[i] çš„è¾“å…¥ç»´åº¦åœ¨ build_network é‡Œå·²ç»æŒ‰ step ç®—å¥½äº†
                reduced_feat = self.HumanSpace_Conv[i](sub_app) # B num_unknown -> B 16 64 32
                
                processed_feat_list.append(reduced_feat) # *4
                
                # é‡Šæ”¾æ˜¾å­˜
                del sub_app
                del sub_feats

            # 3. æ‹¼æ¥æ‰€æœ‰ Head çš„è¾“å‡º
            human_feat = torch.concat(processed_feat_list, dim=1) # B num_unknown*num_FPN H W -> B 16*4=64 64 32
            
            # =======================================================
            # 3. åå¤„ç† (Mask & GaitNet)
            # =======================================================
            
            # ç”Ÿæˆå…¨ 1 Mask (è·³è¿‡ Mask Branch)
            human_mask_ori = torch.ones(
                (n*s, 1, h_feat, h_feat), 
                dtype=human_feat.dtype, 
                device=human_feat.device
            )
            
            # Resize Mask åˆ°ç›®æ ‡å°ºå¯¸
            human_mask = self.preprocess(
                human_mask_ori, 
                self.sils_size*2, 
                self.sils_size
            ).detach()
            
            # åº”ç”¨ Mask
            human_feat = human_feat * (human_mask > 0.5).float()
            
            # Reshape å–‚ç»™ GaitNet
            human_feat = rearrange(human_feat.view(n, s, -1, self.sils_size*2, self.sils_size), 'n s c h w -> n c s h w').contiguous()

            # GaitNet Part 1
            outs = self.Gait_Net.test_1(human_feat)
            all_outs.append(outs)

        # # GaitNet Part 2 (Temporal Aggregation)
        # embed_list, log_list = self.Gait_Net.test_2(
        #     torch.cat(all_outs, dim=2),
        #     seqL,
        # )
        
        # 1. æ‹¼æ¥ç‰¹å¾ (åœ¨æ—¶é—´ç»´åº¦ s ä¸Š, dim=2)
        # [n, c, S_total, h, w]
        feat_total = torch.cat(all_outs, dim=2)
        
        # 2. æ‹¼æ¥ Attention Maps (åœ¨æ—¶é—´ç»´åº¦ s ä¸Š, dim=2)
        # [n, p, S_total, h, w]
        map_total = torch.cat(all_attn_maps, dim=2)
        
        # 3. è°ƒç”¨æ–°çš„ test_2 (Semantic Pooling + Temporal Pooling)
        # ä¼ å…¥ç‰¹å¾å’Œå¯¹åº”çš„ Attention Map
        embed_list, log_list = self.Gait_Net.test_2(
            feat_total,
            map_total, 
            seqL,
        )


        # ç»„è£…è¿”å›å€¼
        if self.training:
            retval = {
                'training_feat': {
                    'triplet': {'embeddings': torch.concat(embed_list, dim=-1), 'labels': labs},
                    'softmax': {'logits': torch.concat(log_list, dim=-1), 'labels': labs},
                },
                'visual_summary': {
                    'image/rgb_img': rgb_img.view(n*s, c, h, w)[:5].float(),
                    'image/human_mask': self.min_max_norm(human_mask.view(n*s, -1, self.sils_size*2, self.sils_size)[:5].float()).clamp(0,1),
                },
                'inference_feat': {
                    'embeddings': torch.concat(embed_list, dim=-1),
                    **{f'embeddings_{i}': embed_list[i] for i in range(self.num_FPN)}
                }
            }
        else:
            retval = {
                'training_feat': {},
                'visual_summary': {},
                'inference_feat': {
                    'embeddings': torch.concat(embed_list, dim=-1),
                    **{f'embeddings_{i}': embed_list[i] for i in range(self.num_FPN)}
                }
            }
        return retval